{
 "metadata": {
  "name": "",
  "signature": "sha256:0a8ead06cc5950197d25238e5ca99b52a859059e0a2d5be370b608f557939cc2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle as pickle\n",
      "from htmlparser import JobListings, Job\n",
      "import numpy as np\n",
      "import re\n",
      "import scipy.sparse\n",
      "import string\n",
      "from sklearn import linear_model\n",
      "from sparsesvd import sparsesvd\n",
      "\n",
      "listings = []\n",
      "with open('joblistings.pkl', 'rb') as f:\n",
      "    listings = pickle.load(f)    \n",
      "len(listings)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "5342"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "sys.path.append('tagger/')\n",
      "import tagger\n",
      "\n",
      "weights = pickle.load(open('tagger/data/dict.pkl', 'rb')) # or your own dictionary\n",
      "# myreader = tagger.Reader() # or your own reader class\n",
      "# mystemmer = tagger.Stemmer() # or your own stemmer class\n",
      "# myrater = tagger.Rater(weights) # or your own... (you got the idea)\n",
      "# mytagger = Tagger(myreader, mystemmer, myrater)\n",
      "# best_3_tags = mytagger(text_string, 3)\n",
      "help(tagger)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Help on module tagger:\n",
        "\n",
        "NAME\n",
        "    tagger\n",
        "\n",
        "FILE\n",
        "    /home/jin/git/workspace/tagger/tagger.py\n",
        "\n",
        "DESCRIPTION\n",
        "    ======\n",
        "    tagger\n",
        "    ======\n",
        "    \n",
        "    Module for extracting tags from text documents.\n",
        "                       \n",
        "    Copyright (C) 2011 by Alessandro Presta\n",
        "    \n",
        "    Configuration\n",
        "    =============\n",
        "    \n",
        "    Dependencies:\n",
        "    python2.7, stemming, nltk (optional), lxml (optional), tkinter (optional)\n",
        "    \n",
        "    You can install the stemming package with::\n",
        "    \n",
        "        $ easy_install stemming\n",
        "    \n",
        "    Usage\n",
        "    =====\n",
        "    \n",
        "    Tagging a text document from Python::\n",
        "    \n",
        "        import tagger\n",
        "        weights = pickle.load(open('data/dict.pkl', 'rb')) # or your own dictionary\n",
        "        myreader = tagger.Reader() # or your own reader class\n",
        "        mystemmer = tagger.Stemmer() # or your own stemmer class\n",
        "        myrater = tagger.Rater(weights) # or your own... (you got the idea)\n",
        "        mytagger = Tagger(myreader, mystemmer, myrater)\n",
        "        best_3_tags = mytagger(text_string, 3)\n",
        "    \n",
        "    Running the module as a script::\n",
        "    \n",
        "        $ ./tagger.py <text document(s) to tag>\n",
        "    \n",
        "    Example::\n",
        "    \n",
        "        $ ./tagger.py tests/*\n",
        "        Loading dictionary... \n",
        "        Tags for  tests/bbc1.txt :\n",
        "        ['bin laden', 'obama', 'pakistan', 'killed', 'raid']\n",
        "        Tags for  tests/bbc2.txt :\n",
        "        ['jo yeates', 'bristol', 'vincent tabak', 'murder', 'strangled']\n",
        "        Tags for  tests/bbc3.txt :\n",
        "        ['snp', 'party', 'election', 'scottish', 'labour']\n",
        "        Tags for  tests/guardian1.txt :\n",
        "        ['bin laden', 'al-qaida', 'killed', 'pakistan', 'al-fawwaz']\n",
        "        Tags for  tests/guardian2.txt :\n",
        "        ['clegg', 'tory', 'lib dem', 'party', 'coalition']\n",
        "        Tags for  tests/post1.txt :\n",
        "        ['sony', 'stolen', 'playstation network', 'hacker attack', 'lawsuit']\n",
        "        Tags for  tests/wikipedia1.txt :\n",
        "        ['universe', 'anthropic principle', 'observed', 'cosmological', 'theory']\n",
        "        Tags for  tests/wikipedia2.txt :\n",
        "        ['beetroot', 'beet', 'betaine', 'blood pressure', 'dietary nitrate']\n",
        "        Tags for  tests/wikipedia3.txt :\n",
        "        ['the lounge lizards', 'jazz', 'john lurie', 'musical', 'albums']\n",
        "\n",
        "CLASSES\n",
        "    Rater\n",
        "    Reader\n",
        "    Stemmer\n",
        "    Tag\n",
        "        MultiTag\n",
        "    Tagger\n",
        "    \n",
        "    class MultiTag(Tag)\n",
        "     |  Class for aggregates of tags (usually next to each other in the document)\n",
        "     |  \n",
        "     |  Methods defined here:\n",
        "     |  \n",
        "     |  __init__(self, tail, head=None)\n",
        "     |      @param tail: the L{Tag} object to add to the first part (head)\n",
        "     |      @param head: the (eventually absent) L{MultiTag} to be extended\n",
        "     |      \n",
        "     |      @returns: a new L{MultiTag} object\n",
        "     |  \n",
        "     |  combined_rating(self)\n",
        "     |      Method that computes the multitag's rating from the ratings of unit\n",
        "     |      subtags\n",
        "     |      \n",
        "     |      (the default implementation uses the geometric mean - with a special\n",
        "     |      treatment for proper nouns - but this method can be overridden)\n",
        "     |      \n",
        "     |      @returns: the rating of the multitag\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Methods inherited from Tag:\n",
        "     |  \n",
        "     |  __eq__(self, other)\n",
        "     |  \n",
        "     |  __hash__(self)\n",
        "     |  \n",
        "     |  __lt__(self, other)\n",
        "     |  \n",
        "     |  __repr__(self)\n",
        "    \n",
        "    class Rater\n",
        "     |  Class for estimating the relevance of tags\n",
        "     |  \n",
        "     |  (the default implementation uses TF (term frequency) multiplied by weight,\n",
        "     |  but any other reasonable measure is fine; a quite rudimental heuristic\n",
        "     |  tries to discard redundant tags)\n",
        "     |  \n",
        "     |  Methods defined here:\n",
        "     |  \n",
        "     |  __call__(self, tags)\n",
        "     |      @param tags: a list of (preferably stemmed) tags\n",
        "     |      \n",
        "     |      @returns: a list of unique (multi)tags sorted by relevance\n",
        "     |  \n",
        "     |  __init__(self, weights, multitag_size=3)\n",
        "     |      @param weights:       a dictionary of weights normalized in the\n",
        "     |                            interval [0,1]\n",
        "     |      @param multitag_size: maximum size of tags formed by multiple unit\n",
        "     |                            tags\n",
        "     |      \n",
        "     |      @returns: a new L{Rater} object\n",
        "     |  \n",
        "     |  create_multitags(self, tags)\n",
        "     |      @param tags: a list of tags (respecting the order in the text)\n",
        "     |      \n",
        "     |      @returns: a list of multitags\n",
        "     |  \n",
        "     |  rate_tags(self, tags)\n",
        "     |      @param tags: a list of tags to be assigned a rating\n",
        "    \n",
        "    class Reader\n",
        "     |  Class for parsing a string of text to obtain tags\n",
        "     |  \n",
        "     |  (it just turns the string to lowercase and splits it according to\n",
        "     |  whitespaces and punctuation, identifying proper nouns and terminal words;\n",
        "     |  different rules and formats other than plain text could be used)\n",
        "     |  \n",
        "     |  Methods defined here:\n",
        "     |  \n",
        "     |  __call__(self, text)\n",
        "     |      @param text: the string of text to be tagged\n",
        "     |      \n",
        "     |      @returns: a list of tags respecting the order in the text\n",
        "     |  \n",
        "     |  preprocess(self, text)\n",
        "     |      @param text: a string containing the text document to perform any\n",
        "     |                   required transformation before splitting\n",
        "     |      \n",
        "     |      @returns:    the processed text\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Data and other attributes defined here:\n",
        "     |  \n",
        "     |  match_apostrophes = <_sre.SRE_Pattern object>\n",
        "     |  \n",
        "     |  match_paragraphs = <_sre.SRE_Pattern object>\n",
        "     |  \n",
        "     |  match_phrases = <_sre.SRE_Pattern object>\n",
        "     |  \n",
        "     |  match_words = <_sre.SRE_Pattern object>\n",
        "    \n",
        "    class Stemmer\n",
        "     |  Class for extracting the stem of a word\n",
        "     |  \n",
        "     |  (by default it uses a simple open-source implementation of Porter's\n",
        "     |  algorithm; this can be improved a lot, so experimenting with different ones\n",
        "     |  is advisable; nltk.stem provides different algorithms for many languages)\n",
        "     |  \n",
        "     |  Methods defined here:\n",
        "     |  \n",
        "     |  __call__(self, tag)\n",
        "     |      @param tag: the tag to be stemmed\n",
        "     |      \n",
        "     |      @returns: the stemmed tag\n",
        "     |  \n",
        "     |  __init__(self, stemmer=None)\n",
        "     |      @param stemmer: an object or module with a 'stem' method (defaults to\n",
        "     |                      stemming.porter2)\n",
        "     |      \n",
        "     |      @returns: a new L{Stemmer} object\n",
        "     |  \n",
        "     |  preprocess(self, string)\n",
        "     |      @param string: a string to be treated before passing it to the stemmer\n",
        "     |      \n",
        "     |      @returns: the processed string\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Data and other attributes defined here:\n",
        "     |  \n",
        "     |  match_contractions = <_sre.SRE_Pattern object>\n",
        "     |  \n",
        "     |  match_hyphens = <_sre.SRE_Pattern object>\n",
        "    \n",
        "    class Tag\n",
        "     |  General class for tags (small units of text)\n",
        "     |  \n",
        "     |  Methods defined here:\n",
        "     |  \n",
        "     |  __eq__(self, other)\n",
        "     |  \n",
        "     |  __hash__(self)\n",
        "     |  \n",
        "     |  __init__(self, string, stem=None, rating=1.0, proper=False, terminal=False)\n",
        "     |      @param string:   the actual representation of the tag\n",
        "     |      @param stem:     the internal (usually stemmed) representation;\n",
        "     |                       tags with the same stem are regarded as equal\n",
        "     |      @param rating:   a measure of the tag's relevance in the interval [0,1]\n",
        "     |      @param proper:   whether the tag is a proper noun\n",
        "     |      @param terminal: set to True if the tag is at the end of a phrase\n",
        "     |                       (or anyway it cannot be logically merged to the\n",
        "     |                       following one)\n",
        "     |      \n",
        "     |      @returns: a new L{Tag} object\n",
        "     |  \n",
        "     |  __lt__(self, other)\n",
        "     |  \n",
        "     |  __repr__(self)\n",
        "    \n",
        "    class Tagger\n",
        "     |  Master class for tagging text documents\n",
        "     |  \n",
        "     |  (this is a simple interface that should allow convenient experimentation\n",
        "     |  by using different classes as building blocks)\n",
        "     |  \n",
        "     |  Methods defined here:\n",
        "     |  \n",
        "     |  __call__(self, text, tags_number=5)\n",
        "     |      @param text:        the string of text to be tagged\n",
        "     |      @param tags_number: number of best tags to be returned\n",
        "     |      \n",
        "     |      Returns: a list of (hopefully) relevant tags\n",
        "     |  \n",
        "     |  __init__(self, reader, stemmer, rater)\n",
        "     |      @param reader: a L{Reader} object\n",
        "     |      @param stemmer: a L{Stemmer} object\n",
        "     |      @param rater: a L{Rater} object\n",
        "     |      \n",
        "     |      @returns: a new L{Tagger} object\n",
        "\n",
        "DATA\n",
        "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import stemming\n",
      "import build_dict\n",
      "\n",
      "help(build_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Help on module build_dict:\n",
        "\n",
        "NAME\n",
        "    build_dict - Usage: build_dict.py -o <output file> -s <stopwords file> <list of files>\n",
        "\n",
        "FILE\n",
        "    /home/jin/git/workspace/tagger/build_dict.py\n",
        "\n",
        "FUNCTIONS\n",
        "    build_dict(corpus, stopwords=None, measure='IDF')\n",
        "        @param corpus:    a list of documents, represented as lists of (stemmed)\n",
        "                          words\n",
        "        @param stopwords: the list of (stemmed) words that should have zero weight\n",
        "        @param measure:   the measure used to compute the weights ('IDF'\n",
        "                          i.e. 'inverse document frequency' or 'ICF' i.e.\n",
        "                          'inverse collection frequency'; defaults to 'IDF')\n",
        "        \n",
        "        @returns: a dictionary of weights in the interval [0,1]\n",
        "    \n",
        "    build_dict_from_files(output_file, corpus_files, stopwords_file=None, reader=<extras.SimpleReader instance>, stemmer=<tagger.Stemmer instance>, measure='IDF', verbose=False)\n",
        "        @param output_file:    the name of the file where the dictionary should be\n",
        "                               saved\n",
        "        @param corpus_files:   a list of files with words to process\n",
        "        @param stopwords_file: a file containing a list of stopwords\n",
        "        @param reader:         the L{Reader} object to be used\n",
        "        @param stemmer:        the L{Stemmer} object to be used\n",
        "        @param measure:        the measure used to compute the weights ('IDF'\n",
        "                               i.e. 'inverse document frequency' or 'ICF' i.e.\n",
        "                               'inverse collection frequency'; defaults to 'IDF')\n",
        "        @param verbose:        whether information on the progress should be\n",
        "                               printed on screen\n",
        "\n",
        "DATA\n",
        "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stop = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
      "        'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself',\n",
      "        'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',\n",
      "        'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be',\n",
      "        'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an',\n",
      "        'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by',\n",
      "        'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n",
      "        'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over',\n",
      "        'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',\n",
      "        'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no',\n",
      "        'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will',\n",
      "        'just', 'don', 'should', 'now', 'skills']\n",
      "\n",
      "weights = pickle.load(open('jobtagdict.pkl', 'rb')) # or your own dictionary\n",
      "# weights2 = pickle.load(open('skilltagdict.pkl', 'rb'))\n",
      "weights2 = pickle.load(open('tagger/data/dict.pkl', 'rb'))\n",
      "myreader = tagger.Reader() # or your own reader class\n",
      "mystemmer = tagger.Stemmer() # or your own stemmer class\n",
      "myrater = tagger.Rater(weights) # or your own... (you got the idea)\n",
      "myrater2 = tagger.Rater(weights2)\n",
      "jobtagger = tagger.Tagger(myreader, mystemmer, myrater)\n",
      "skilltagger = tagger.Tagger(myreader, mystemmer, myrater2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(\"stopwords.pkl\") as f:\n",
      "    l = JobListings(f.read())\n",
      "    save_object(l.listings, r'joblistings-cs.pkl')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('jobtagcorpus.txt', 'wb') as f:\n",
      "    for job in listings:\n",
      "        f.write(job.desc)\n",
      "        if getattr(job, 'qualifications', False):\n",
      "            f.write(job.qualifications)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('skilltagcorpus.txt', 'wb') as f:\n",
      "    for job in listings:\n",
      "        if getattr(job, 'qualifications', False):\n",
      "            f.write(job.qualifications)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "build_dict.build_dict_from_files('skilltagdict.pkl', ['skilltagcorpus.txt'], stopwords_file='stopwords.txt', measure='ICF')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "build_dict.build_dict_from_files('jobtagdict.pkl', ['jobtagcorpus.txt'], stopwords_file='stopwords.txt', measure='ICF')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text_string = listings[5].qualifications\n",
      "jobtagger(text_string, 3)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 53,
       "text": [
        "['strong advantage', 'enthusiasm', 'strong analytical']"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "j = listings[1]\n",
      "text_string = j.desc + '\\n' + j.qualifications\n",
      "q = jobtagger(text_string, 5)\n",
      "[repr(x)[1:-1] for x in q]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 89,
       "text": [
        "['unilever', 'customer', 'development', 'business', 'marketing']"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(listings)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 90,
       "text": [
        "5342"
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle as pickle\n",
      "\n",
      "### mutate jobs\n",
      "for job in listings:\n",
      "    text_string = job.desc\n",
      "    tags = [repr(x)[1:-1] for x in jobtagger(text_string, 5)]\n",
      "    setattr(job, 'tags', tags)\n",
      "    if getattr(job, 'qualifications', False):\n",
      "        text_string = job.qualifications\n",
      "        skills = [repr(x)[1:-1] for x in jobtagger(text_string, 3)]\n",
      "        setattr(job, 'skills', skills)\n",
      "    else:\n",
      "        setattr(job, 'qualifications', '')\n",
      "        setattr(job, 'skills', '')\n",
      "        \n",
      "def save_object(obj, filename):\n",
      "    with open(filename, 'wb') as output:\n",
      "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
      "        \n",
      "with open(r'joblistings-tagged.pkl', 'wb') as f:    \n",
      "    pickle.dump(listings, f, pickle.HIGHEST_PROTOCOL)\n",
      "   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "argument must have 'write' attribute",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-91-47ad1f15b2eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'joblistings-tagged.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlistings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mr'joblistings-tagged.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mTypeError\u001b[0m: argument must have 'write' attribute"
       ]
      }
     ],
     "prompt_number": 91
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(r'joblistings-tagged.pkl', 'wb') as f:    \n",
      "    pickle.dump(listings, f, pickle.HIGHEST_PROTOCOL)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 93
    }
   ],
   "metadata": {}
  }
 ]
}